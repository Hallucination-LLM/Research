{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2ed640-f9c4-409f-8cf7-7c5c6f836c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U torch\n",
    "# !pip install accelerate\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deaf96c3-20f1-4f0b-ab16-4b6fe8d00067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from getpass import getpass\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d6e01e5-d877-43b2-910b-b79274a8a638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c013e302-6bbc-4cdb-85f7-f31fbaa1ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "hf_token = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef19137-9bcc-4a9c-abc0-55197d1965f0",
   "metadata": {},
   "source": [
    "## Extracting lookback-ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc305b87-acb3-4453-b578-14ab7d3e4417",
   "metadata": {},
   "source": [
    "Original Lookback-Ratio code is Llama-specific so I wrote a script which takes model, tokenizer and a list of texts and then generates the answers and extracts lookback-ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d708d4-0d26-49d3-9807-db70a81b2817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005301713943481445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f8c24724a242468dfa886ed973c8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "llama2_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "llama2_tokenizer = AutoTokenizer.from_pretrained(llama2_model_name, token=hf_token)\n",
    "llama2_model = AutoModelForCausalLM.from_pretrained(llama2_model_name, device_map=\"auto\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08e5c29-d202-4b66-bc89-2d71daff484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Write me a poem about Machine Learning. Be creative!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "398ef7ac-ba56-453d-b5bd-f81ff9a1f13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = llama2_tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e292f143-8fc7-40e6-a26c-c789148205c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "outputs = llama2_model.generate(**input_ids, max_new_tokens=32, output_attentions=True, return_dict_in_generate=True)\n",
    "\n",
    "attentions = outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace2b67e-93d7-48d2-9cb4-522d8bc2910e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, torch.Size([1, 32, 13, 13]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attentions), len(attentions[0]), attentions[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2caacc6-1f03-4859-9d44-c311f4adeebd",
   "metadata": {},
   "source": [
    "```attentions``` is a tuple (32 - one element per generated token) of tuples (32 - one element per layer) of tensors (1 - batch_size, 32 - num_heads, 13 - generated_length, 13 - sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8e5ebea-19aa-4b1f-9597-fd9829809299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 14])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424241fd-4339-41b4-955c-a77eabe388e1",
   "metadata": {},
   "source": [
    "But for tokens other than the first the shape of the tensor is different. The 3rd dimension is always 1, because we are generating one token at a time, and the 4th dimension is always 1 element longer then for the preceding token. For ```attentions[2][0].shape``` it will be 16 and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e949484-588c-4159-9968-cdde3fd015e5",
   "metadata": {},
   "source": [
    "**However those shapes differ between models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a09c9fb-b58a-48c0-b157-41c472370780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12132"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del llama2_model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cc136b7-b020-49a1-bf5f-24c6ad43961e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0052640438079833984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5235a10e0f344e48af90dd257dad6b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma2_model_name = \"google/gemma-2-2b-it\"\n",
    "\n",
    "gemma2_tokenizer = AutoTokenizer.from_pretrained(gemma2_model_name, token=hf_token)\n",
    "gemma2_model = AutoModelForCausalLM.from_pretrained(gemma2_model_name, device_map=\"auto\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4da4f37-6fec-4813-b00d-62be86a56591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = gemma2_tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9d9237d-e314-478a-964c-5b6dea9b003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = gemma2_model.generate(**input_ids, max_new_tokens=32, output_attentions=True, return_dict_in_generate=True)\n",
    "\n",
    "attentions = outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8da5c06c-106b-4005-98a8-62b7353ce67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 26, torch.Size([1, 8, 12, 44]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attentions), len(attentions[0]), attentions[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0de37eb-bfc0-446a-862a-a8dc92f456d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1, 44])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56569b4b-cfb4-4134-8d9f-63c875b17ca5",
   "metadata": {},
   "source": [
    "In this case the last dimension of the tensor is always 44 (12 tokens in prompt + 32 generated). It is not getting longer for every single token. Elements for the tokens that were not generated yet have values of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d1227-4076-4b4d-ad1c-e7ba89729a7c",
   "metadata": {},
   "source": [
    "Function below is designed to extract lookback-ratio despite those differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4fab4a3-a509-47d2-8e7f-9a07e4ef7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lbrs(model, tokenizer, input_texts):\n",
    "  results = []\n",
    "\n",
    "  for input_text in input_texts:\n",
    "    print(50*\"=\")\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    print(\"input_ids.input_ids.shape\", input_ids.input_ids.shape)\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=32, output_attentions=True, return_dict_in_generate=True)\n",
    "\n",
    "    attentions = outputs.attentions\n",
    "    new_token_length = len(attentions)\n",
    "    n_layers = len(attentions[0])\n",
    "    n_heads = attentions[0][0].shape[1]\n",
    "\n",
    "    print(\"new_token_length: {}, n_layers: {}, n_heads: {}\".format(new_token_length, n_layers, n_heads))\n",
    "\n",
    "    prompt_length = input_ids.input_ids.shape[1]\n",
    "    print(\"prompt_length\", prompt_length)\n",
    "\n",
    "    lbr = torch.zeros(n_layers, n_heads, new_token_length).to(device)\n",
    "\n",
    "    for t in range(new_token_length):\n",
    "      for l in range(n_layers):\n",
    "        attn_on_context = attentions[t][l][0, :, -1, :prompt_length]\n",
    "        avg_attn_on_context = attn_on_context.mean(-1)\n",
    "\n",
    "        attn_on_new = attentions[t][l][0, :, -1, prompt_length:prompt_length+t]\n",
    "        avg_attn_on_new = attn_on_new.mean(-1) if attn_on_new.shape[-1] > 0 else torch.zeros(attn_on_new.shape[0]).to(device)\n",
    "\n",
    "        lbr[l, :, t] = avg_attn_on_context / (avg_attn_on_context + avg_attn_on_new)\n",
    "    print(\"lbr.shape\", lbr.shape)\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=False)[0]\n",
    "    print(\"decoded\", decoded)\n",
    "\n",
    "    results.append({\n",
    "        \"input_text\": input_text,\n",
    "        \"decoded\": decoded,\n",
    "        \"lbr\": lbr\n",
    "    })\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf58de3e-ff8f-4459-b45c-f76efaa3f289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "input_ids.input_ids.shape torch.Size([1, 12])\n",
      "new_token_length: 32, n_layers: 26, n_heads: 8\n",
      "prompt_length 12\n",
      "lbr.shape torch.Size([26, 8, 32])\n",
      "decoded <bos>Write me a poem about Machine Learning. Be creative!\n",
      "\n",
      "In silicon valleys, where code takes flight,\n",
      "A mind of metal, bathed in digital light.\n",
      "Machine learning, a whisper in the breeze,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lbrs = extract_lbrs(gemma2_model, gemma2_tokenizer, [input_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "777178b2-69c4-49cc-b9ac-19b58f21d271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 8, 32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbrs[0][\"lbr\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d918c-8f4d-4283-9299-a162887f3f5f",
   "metadata": {},
   "source": [
    "26 layers, 8 heads and lookback-ratio for each generated token (32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f168cff0-c3e3-4f80-bee0-c5b48c9a3219",
   "metadata": {},
   "source": [
    "For example, lookback-ratio for first layer, first head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ad4af06-7024-4082-bc21-f51e54e87c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.8250, 0.0315, 0.1540, 0.0871, 0.0709, 0.0543, 0.1421, 0.1626,\n",
       "        0.0849, 0.1643, 0.0702, 0.0952, 0.2524, 0.0533, 0.4463, 0.1824, 0.5189,\n",
       "        0.0738, 0.4419, 0.2973, 0.5011, 0.1802, 0.3557, 0.2904, 0.2679, 0.1703,\n",
       "        0.5507, 0.1415, 0.1690, 0.4247, 0.3822], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbrs[0][\"lbr\"][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521a6f5-234b-46f7-a1e1-391862b6c820",
   "metadata": {},
   "source": [
    "**Side note:** Lookback-ratio for the first token is exactly 1, because it is attention_on_context / (attention_on_context + attention_on_new). As it is the first token attention_on_new is 0. However in the original Lookback-Ratio implementation the value for the first token is not equal to 1. The reason is that in the original implementation when the prompt is built some extra text is appended at the end, for example ```\"\\n#Summary#:\"``` for the task of summarization. Than during calculating lookback-ratio those extra tokens are being trated as if they were generated by the model, although they were not actually. But this is the reason why lookback-ratio is not equal to 1, although it should be according to the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9fc4bf-41fb-4948-99c1-cec3051e28d3",
   "metadata": {},
   "source": [
    "## Modifying attention \"by hand\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50d5cc-aaab-42ab-8a70-3012f1d2e299",
   "metadata": {},
   "source": [
    "1. I tried to use torch hooks (model.register_forward_hook()). However it is a function that is called **after** forward is called. So even though we can get attention maps, the token was already generated.\n",
    "2. Another approach is to modify transformers library code but it requires more time, is prone to bugs and is not model agnostic so I gave it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a443a-f7de-4c1b-bd2d-2c4cb21ed1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
