{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xsum = pd.read_parquet(\"gemma_att_kl_0_7000_df.parquet\")\n",
    "df_cnn = pd.read_parquet(\"cnn_att_kl_0_1000_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
       "       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',\n",
       "       '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36',\n",
       "       '37', '38', '39', '40', 'label', 'dataset'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xsum.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"train_name\": \"xsum\",\n",
    "        \"train\": df_xsum,\n",
    "        \"test\": df_cnn\n",
    "    },\n",
    "    {\n",
    "        \"train_name\": \"cnn\",\n",
    "        \"train\": df_cnn,\n",
    "        \"test\": df_xsum\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    # kfold validation over the dataset\n",
    "    df_train = dataset['train']\n",
    "    df_test = dataset['test']\n",
    "\n",
    "    X_train = df_train.drop(columns=['label', 'dataset'])\n",
    "    y_train = df_train['label']\n",
    "\n",
    "    X_test = df_test.drop(columns=['label', 'dataset'])\n",
    "    y_test = df_test['label']\n",
    "\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=10000, class_weight='balanced', random_state=42),\n",
    "        'LGBMClassifier': LGBMClassifier(\n",
    "            n_estimators=25,\n",
    "            learning_rate=0.005,\n",
    "            max_depth=5,\n",
    "            num_leaves=7,\n",
    "            class_weight='balanced',\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            silent=True,\n",
    "            verbose=-1\n",
    "        )\n",
    "    }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "        results.append({\n",
    "            'dataset': dataset['train_name'],\n",
    "            'model': model_name,\n",
    "            'train_auc': roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xsum</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.709079</td>\n",
       "      <td>0.637637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xsum</td>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.631963</td>\n",
       "      <td>0.587371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnn</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.754208</td>\n",
       "      <td>0.603219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn</td>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.769693</td>\n",
       "      <td>0.544042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset               model  train_auc   roc_auc\n",
       "0    xsum  LogisticRegression   0.709079  0.637637\n",
       "1    xsum      LGBMClassifier   0.631963  0.587371\n",
       "2     cnn  LogisticRegression   0.754208  0.603219\n",
       "3     cnn      LGBMClassifier   0.769693  0.544042"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning as L\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchmetrics.classification import AUROC\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "# 1. Create a Synthetic Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# 2. Define the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch_size, seq_len, input_dim)\n",
    "        _, (hn, _) = self.lstm(x)  # hn is (num_layers, batch_size, hidden_dim)\n",
    "        out = self.fc(hn[-1])  # Use the last layer's hidden state\n",
    "        return out\n",
    "\n",
    "# 3. Create the Lightning Module\n",
    "class LSTMClassifier(L.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=5e-4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.model = LSTMModel(input_dim, hidden_dim, output_dim, num_layers=num_layers)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "\n",
    "        # AUROC metric for binary classification\n",
    "        self.train_auc = AUROC(task=\"binary\")\n",
    "        self.val_auc = AUROC(task=\"binary\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        preds = self(data)\n",
    "        loss = self.criterion(preds, labels)\n",
    "        # Calculate AUC during training\n",
    "        prob = torch.softmax(preds, dim=1)[:, 1]  # Take probabilities for class 1\n",
    "        auc = self.train_auc(prob, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_auc\", auc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        preds = self(data)\n",
    "        loss = self.criterion(preds, labels)\n",
    "        # Calculate AUC during validation\n",
    "        prob = torch.softmax(preds, dim=1)[:, 1]  # Take probabilities for class 1\n",
    "        auc = self.val_auc(prob, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_auc\", auc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | LSTMModel        | 60.8 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "2 | train_auc | BinaryAUROC      | 0      | train\n",
      "3 | val_auc   | BinaryAUROC      | 0      | train\n",
      "-------------------------------------------------------\n",
      "60.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.8 K    Total params\n",
      "0.243     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/plgkonkie311/miniconda3/envs/hallu/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/plgkonkie311/miniconda3/envs/hallu/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/244 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/plgkonkie311/miniconda3/envs/hallu/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 244/244 [00:03<00:00, 75.17it/s, v_num=23, val_loss=0.644, val_auc=0.543, train_loss=0.667, train_auc=0.291]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 244/244 [00:05<00:00, 43.56it/s, v_num=23, val_loss=0.544, val_auc=0.510, train_loss=0.578, train_auc=0.337]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.100 >= min_delta = 0.0. New best score: 0.544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 244/244 [00:07<00:00, 33.64it/s, v_num=23, val_loss=0.446, val_auc=0.523, train_loss=0.436, train_auc=0.317]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.098 >= min_delta = 0.0. New best score: 0.446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 244/244 [00:09<00:00, 26.26it/s, v_num=23, val_loss=0.389, val_auc=0.523, train_loss=0.333, train_auc=0.294]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 0.389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 244/244 [00:11<00:00, 21.87it/s, v_num=23, val_loss=0.360, val_auc=0.521, train_loss=0.263, train_auc=0.329]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.029 >= min_delta = 0.0. New best score: 0.360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 244/244 [00:13<00:00, 18.37it/s, v_num=23, val_loss=0.346, val_auc=0.522, train_loss=0.222, train_auc=0.318]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 244/244 [00:14<00:00, 16.53it/s, v_num=23, val_loss=0.339, val_auc=0.521, train_loss=0.194, train_auc=0.336]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 244/244 [00:17<00:00, 14.19it/s, v_num=23, val_loss=0.337, val_auc=0.523, train_loss=0.172, train_auc=0.325]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 244/244 [00:26<00:00,  9.32it/s, v_num=23, val_loss=0.358, val_auc=0.525, train_loss=0.133, train_auc=0.318]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 4 records. Best score: 0.337. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 244/244 [00:26<00:00,  9.31it/s, v_num=23, val_loss=0.358, val_auc=0.525, train_loss=0.133, train_auc=0.318]\n"
     ]
    }
   ],
   "source": [
    "X_train = df_cnn.drop(columns=['label', 'dataset'])\n",
    "y_train = df_cnn['label']\n",
    "\n",
    "X_val = df_xsum.drop(columns=['label', 'dataset'])\n",
    "y_val = df_xsum['label']\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 4. Train the Model\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = 2\n",
    "\n",
    "\n",
    "model = LSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, lr=1e-5, num_layers=2)\n",
    "\n",
    "# 1. Set up EarlyStopping callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # The metric to monitor\n",
    "    patience=4,          # Number of epochs with no improvement after which training will stop\n",
    "    verbose=True,        # Display a message when stopping\n",
    "    mode=\"min\",          # Minimize the monitored metric (for loss)\n",
    ")\n",
    "\n",
    "# 2. Initialize the Trainer with the early stopping callback\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[early_stop_callback],  # Add the early stopping callback here\n",
    ")\n",
    "\n",
    "# 3. Start the training\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
