{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_att_pipe(\n",
    "        att_tensor: np.ndarray,\n",
    "        n_first_tokens: int = None,\n",
    "        skip_first_n_tokens: int = None,\n",
    "        skip_last_n_tokens: int = None,\n",
    "        n_context_tokens_start_idx: int = None,\n",
    "        n_context_tokens_end_idx: int = None,\n",
    "        window_size: int = 0,\n",
    "        window_step: int = 4,\n",
    "        postprocess_fn: callable = None,\n",
    "        valid_example_th: int = 4,\n",
    "        **kwargs: dict,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to prepare the attention tensor for further analysis.\n",
    "        It removes the prompt tokens and the last offset_size tokens from the context.\n",
    "        \"\"\"\n",
    "\n",
    "        skip_first_n_tokens = skip_first_n_tokens if skip_first_n_tokens is not None else 0\n",
    "        skip_last_n_tokens = skip_last_n_tokens if skip_last_n_tokens is not None else 0\n",
    "        n_first_tokens = n_first_tokens if n_first_tokens is not None else att_tensor.shape[-2]\n",
    "\n",
    "        att_tensor = att_tensor[..., slice(skip_first_n_tokens, n_first_tokens + skip_first_n_tokens - skip_last_n_tokens), slice(n_context_tokens_start_idx, n_context_tokens_end_idx)]\n",
    "        \n",
    "        if att_tensor.shape[-2] < valid_example_th:\n",
    "            return None\n",
    "\n",
    "        if (window_size) and (att_tensor.shape[-2] > window_size):\n",
    "\n",
    "            att_tensor = {\n",
    "                tuple([i, i + window_size]) : postprocess_fn(att_tensor[..., i: i + window_size, :], **kwargs) if kwargs else postprocess_fn(att_tensor[..., i: i + window_size, :])\n",
    "                for i in range(0, att_tensor.shape[-2], window_step) if i + window_size <= att_tensor.shape[-2]\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            att_tensor = postprocess_fn(att_tensor, **kwargs) if kwargs else postprocess_fn(att_tensor)\n",
    "\n",
    "        return att_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jensenshannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_values_context = np.random.randint(100, 500, size=10)\n",
    "random_values_gen = np.random.randint(6, 200, size=10)\n",
    "\n",
    "mock_at = [\n",
    "    np.random.random(size=(42, 16, rvc, 8)) for rvc, rgg in zip(random_values_context, random_values_gen)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_div_heads_agg(att_tensor: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to calculate the distance between the attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    n_layers, n_heads, _, n_generated_tokens = att_tensor.shape\n",
    "\n",
    "    att_tensor = np.concatenate((att_tensor, 1 - np.sum(att_tensor, axis=2, keepdims=True)), axis=2)\n",
    "    reference_distribution = np.mean(att_tensor, axis=1)\n",
    "\n",
    "    js_divergence = np.zeros((n_layers, n_heads, n_generated_tokens))\n",
    "\n",
    "    for i in range(n_heads):\n",
    "        js_divergence[:, i, :] = jensenshannon(att_tensor[:, i, ...], reference_distribution, axis=1)\n",
    "\n",
    "    del att_tensor, reference_distribution\n",
    "    js_divergence = np.mean(js_divergence, axis=-1)\n",
    "\n",
    "    return js_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 16, 256, 8)\n",
      "(42, 256, 8)\n",
      "(42, 16)\n"
     ]
    }
   ],
   "source": [
    "for att in mock_at:\n",
    "\n",
    "    n_layers, n_heads, n_context_tokens, n_generated_tokens = att.shape\n",
    "    # n_bins = int(2 * (n_context_tokens) ** (1/3))\n",
    "    \n",
    "    # quantiles = np.transpose(np.percentile(att, np.linspace(0, 100, n_bins + 1), axis=2), (1, 2, 0, 3))\n",
    "\n",
    "    # binned_data = np.zeros_like(att, dtype=int)\n",
    "\n",
    "    # for i in range(n_layers):\n",
    "    #     for j in range(n_heads):\n",
    "    #         for k in range(n_generated_tokens):\n",
    "    #             binned_data[i, j, :, k] = np.digitize(att[i, j, :, k], quantiles[i, j, :, k])\n",
    "\n",
    "    \n",
    "    # for i in range(n_heads):\n",
    "    # unique, counts = np.unique(binned_data[0, 0, :, 0], return_counts=True)\n",
    "    # print(dict(zip(unique, counts)))\n",
    "    att_full = np.concatenate((att, 1 - np.sum(att, axis=2, keepdims=True)), axis=2)\n",
    "\n",
    "    reference_distribution = np.mean(att_full, axis=1)\n",
    "\n",
    "    print(att_full.shape)\n",
    "    print(reference_distribution.shape)\n",
    "\n",
    "    # heads_probs = np.apply_along_axis(lambda x: np.histogram(x, bins=n_bins, density=True)[0], axis=2, arr=binned_data)\n",
    "    # reference_probs = np.apply_along_axis(lambda x: np.histogram(x, bins=n_bins, density=True)[0], axis=1, arr=reference_distribution)\n",
    "\n",
    "    # # print(heads_probs[0, :, 0])\n",
    "\n",
    "    # # print(heads_probs.shape)\n",
    "    # # print(reference_probs.shape)\n",
    "    \n",
    "\n",
    "    js_divergence = np.zeros((n_layers, n_heads, n_generated_tokens))\n",
    "\n",
    "    for i in range(n_heads):\n",
    "        js_divergence[:, i, :] = jensenshannon(att_full[:, i, ...], reference_distribution, axis=1)\n",
    "\n",
    "    js_divergence = np.mean(js_divergence, axis=-1)\n",
    "\n",
    "    print(js_divergence.shape)\n",
    "    break\n",
    "\n",
    "    # print(js_divergence.shape)\n",
    "    # break\n",
    "\n",
    "    # for each token and layer, calculate the shanon divergence between the head distribution and the reference distribution\n",
    "\n",
    "    # np.apply_along_axis(lambda x: jensenshannon(x, reference_distribution), axis=1, arr=binned_data)\n",
    "\n",
    "    # print(binned_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.apply_along_axis(lambda x: np.histogram(x, bins=n_bins, range=(0, n_bins), density=True)[0], axis=2, arr=binned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 14, 8)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example array with shape (channels, height, width)\n",
    "array = np.random.rand(3, 64, 64)\n",
    "\n",
    "# Change the order of channels to (height, width, channels)\n",
    "transposed_array = np.transpose(array, (1, 2, 0))\n",
    "\n",
    "print(transposed_array.shape)  # Output: (64, 64, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
